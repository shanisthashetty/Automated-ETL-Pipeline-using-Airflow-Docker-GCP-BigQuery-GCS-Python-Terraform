<h1>ğŸ—ï¸ HR Attrition &amp; Headcount Data Engineering Pipeline</h1>
<h3>Automated ETL using Airflow, Docker, GCP (BigQuery + GCS), Python &amp; Terraform</h3>

<p>
This project automates the <b>ingestion, transformation, storage, and visualization</b> of HR Headcount (HC) and Attrition data for a large IT organization.  
The goal is to ensure that management always has an <b>up-to-date analytics dashboard</b> with insights on employee attrition, retention, and workforce distribution.
</p>

<p>
The pipeline orchestrates data extraction, ETL, cloud storage, BigQuery loading, and dashboard refreshes using <b>Apache Airflow</b> running on <b>Docker</b>.  
A complete HR analytics dashboard is built in <b>Looker Studio</b> on top of <b>BigQuery</b>.
</p>

<br>

<h2>ğŸ“ Repository Contents</h2>

<table>
  <tr><th>File</th><th>Description</th></tr>
  <tr><td><b>docker-compose.yaml</b></td><td>Airflow environment running on Docker (scheduler, webserver, workers, PostgreSQL, etc.).</td></tr>
  <tr><td><b>main.tf</b></td><td>Terraform script used to create the Google Cloud Storage (GCS) bucket and related resources.</td></tr>
  <tr><td><b>DE shanistha (1).pdf</b></td><td>Full project documentation with architecture diagrams, screenshots, and detailed explanation.</td></tr>
  <tr><td><b>README.md</b></td><td>Project documentation (this file).</td></tr>
</table>

<br>

<h2>ğŸ“Œ Project Overview</h2>

<p>A large IT company is experiencing rising employee attrition at a specific location. Management needs an automated, reliable dashboard to monitor:</p>

<ul>
  <li>ğŸ‘¥ Headcount (HC)</li>
  <li>ğŸ“‰ Attrition trends over time</li>
  <li>â“ Reasons for attrition</li>
  <li>ğŸ¢ Department-level breakdowns</li>
  <li>ğŸ“Š Distribution of workforce across segments</li>
</ul>

<p>
HR regularly provides fresh CSV data, and this project ensures that the underlying data warehouse and dashboard update <b>automatically</b>, with <b>no manual data processing</b>.
</p>

<br>

<h2>ğŸ”¥ Tools &amp; Technologies</h2>

<ul>
  <li>ğŸŒ€ <b>Apache Airflow</b> (Dockerized) â€“ workflow orchestration</li>
  <li>ğŸ <b>Python</b> â€“ ETL processing &amp; data transformation</li>
  <li>â˜ï¸ <b>Google Cloud Storage (GCS)</b> â€“ raw &amp; processed data storage</li>
  <li>ğŸ—„ï¸ <b>Google BigQuery</b> â€“ data warehouse for analytics</li>
  <li>ğŸ“Š <b>Looker Studio</b> â€“ interactive HR dashboards</li>
  <li>ğŸ§± <b>Terraform</b> â€“ infrastructure as code (GCS bucket, configs)</li>
  <li>ğŸ³ <b>Docker</b> â€“ containerized Airflow deployment</li>
</ul>

<br>

<h2>ğŸ—ï¸ Architecture Overview</h2>

<p>The solution is built as an automated ETL pipeline with the following high-level stages:</p>

<ol>
  <li>ğŸ“¥ <b>Data Ingestion</b> â€“ Fetch HR CSV files from source server</li>
  <li>ğŸ§® <b>ETL Processing</b> â€“ Clean &amp; transform data with Python</li>
  <li>â˜ï¸ <b>Cloud Storage</b> â€“ Store processed files in GCS</li>
  <li>ğŸ—„ï¸ <b>Data Warehouse</b> â€“ Load data into BigQuery tables</li>
  <li>ğŸ“Š <b>Reporting Layer</b> â€“ Visualize metrics in Looker Studio</li>
</ol>

<br>

<h2>ğŸ› ï¸ End-to-End Workflow (Airflow DAG)</h2>

<p>The Airflow DAG consists of the following tasks:</p>

<h3>â–¶ï¸ Task 1 â€” Download CSV from Server (BashOperator)</h3>
<ul>
  <li>Fetch the latest HR CSV file from an internal file server or shared location.</li>
  <li>Rename the file dynamically using the execution date (e.g., <code>20240201.csv</code>).</li>
</ul>

<h3>â–¶ï¸ Task 2 â€” ETL Processing (PythonOperator)</h3>
<ul>
  <li>Convert categorical â†’ numerical fields where needed (e.g., Yes/No, attrition flags).</li>
  <li>Remove unnecessary columns.</li>
  <li>Standardize column names and formats.</li>
  <li>Create a cleaned file named <code>employee_hr_data_YYYYMMDD.csv</code>.</li>
</ul>

<h3>â–¶ï¸ Task 3 â€” Upload to GCS (PythonOperator)</h3>
<ul>
  <li>Upload the cleaned CSV to a designated <b>GCS bucket</b>.</li>
  <li>Use folder structure or naming conventions for partitioning and history tracking.</li>
</ul>

<h3>â–¶ï¸ Task 4 â€” Create BigQuery Table If Not Exists</h3>
<ul>
  <li>Check if the BigQuery destination table exists.</li>
  <li>If not, create it using the schema inferred or predefined in the DAG.</li>
</ul>

<h3>â–¶ï¸ Task 5 â€” Load Data from GCS â†’ BigQuery</h3>
<ul>
  <li>Load the latest processed CSV file from GCS into BigQuery.</li>
  <li>Append data to historical records.</li>
  <li>Ensure the dashboard always reflects the latest HR information.</li>
</ul>

<p>The Airflow UI shows all tasks executing successfully in sequence, validating the pipeline orchestration.</p>

<br>

<h2>â˜ï¸ Google Cloud Setup with Terraform</h2>

<p><b>Terraform</b> is used to provision the necessary GCP infrastructure, primarily the GCS bucket.</p>

<p>The <code>main.tf</code> file includes:</p>

<ul>
  <li>Provider configuration for Google Cloud</li>
  <li>Variables for:
    <ul>
      <li><code>bucket_name</code></li>
      <li><code>bucket_location</code></li>
      <li><code>project_id</code></li>
      <li><code>storage_class</code></li>
    </ul>
  </li>
  <li>A <code>google_storage_bucket</code> resource such as:</li>
</ul>

<pre><code>resource "google_storage_bucket" "hr_bucket" {
  name          = var.bucket_name
  location      = var.bucket_location
  storage_class = var.storage_class
}
</code></pre>

<p>
Once applied, Terraform creates the storage bucket used by Airflow to store processed HR CSVs.
</p>

<br>

<h2>ğŸ—„ï¸ BigQuery Integration</h2>

<ul>
  <li>Airflow loads processed data directly from GCS into a BigQuery table.</li>
  <li>The table is created automatically on the first run if it does not already exist.</li>
  <li>Subsequent DAG runs append new HR records, maintaining a historical dataset.</li>
</ul>

<p>
This setup enables fast analytical queries and supports multiple dashboards &amp; reports.
</p>

<br>

<h2>ğŸ“Š Looker Studio Dashboard</h2>

<p>A complete HR analytics dashboard is built in <b>Looker Studio</b> on top of BigQuery.</p>

<h3>Dashboards Include:</h3>

<ul>
  <li>ğŸ‘¥ <b>Active Headcount Overview</b> â€“ department, gender, age bands, tenure</li>
  <li>ğŸ“‰ <b>Attrition Overview</b> â€“ overall attrition rate, monthly trends</li>
  <li>ğŸ” <b>Attrition Deep Dives</b> â€“ performance rating vs attrition, job role, satisfaction metrics</li>
  <li>ğŸŒ <b>Location &amp; department segmentation</b></li>
</ul>

<p>
Charts used include pie charts, bar charts, and trend lines to help stakeholders quickly interpret HR metrics.
</p>

<br>

<h2>ğŸ¯ Results &amp; Business Impact</h2>

<ul>
  <li>âœ… <b>Fully automated ETL pipeline</b><br>
      No manual steps. HR team simply uploads or refreshes the CSV â†’ Dashboard updates automatically.</li>
  <li>âœ… <b>Cloud-native &amp; scalable</b><br>
      GCS + BigQuery provide reliable, scalable storage and fast analytics.</li>
  <li>âœ… <b>Improved HR decision-making</b><br>
      Management can monitor:
      <ul>
        <li>Attrition by department</li>
        <li>Reasons for leaving</li>
        <li>High-risk employee groups</li>
        <li>Headcount distribution and trends</li>
      </ul>
  </li>
  <li>âœ… <b>Production-ready design</b><br>
      Airflow ensures workflows are repeatable, testable, and maintainable.</li>
</ul>

<br>

<h2>ğŸš€ Future Enhancements</h2>

<ul>
  <li>ğŸ“§ Add email or Slack alerts when attrition crosses thresholds</li>
  <li>â˜¸ Deploy Airflow on Kubernetes for higher scalability &amp; resilience</li>
  <li>ğŸ“ Integrate <b>dbt</b> for modular data modeling in BigQuery</li>
  <li>ğŸ§¬ Add Slowly Changing Dimensions (SCD) to track employee lifecycle changes</li>
  <li>ğŸ¤– Integrate predictive modeling to forecast attrition risk</li>
</ul>

<br>

<h2>â­ Summary</h2>

<p>
This project demonstrates a complete <b>Data Engineering pipeline</b> combining:
</p>

<ul>
  <li>âœ” Airflow orchestration</li>
  <li>âœ” Dockerized infrastructure</li>
  <li>âœ” Cloud storage &amp; data warehousing on GCP</li>
  <li>âœ” Automated ETL for HR data</li>
  <li>âœ” Real-time dashboards for HR analytics</li>
</ul>

<p>
This project is a strong example of my <b>production-grade data engineering</b> for HR analytics and business intelligence.
</p>
